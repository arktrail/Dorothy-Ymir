# Dorothy AI Patent Classifier
This github repository includes code for Dorothy AI Patent Classifier
## Table of Contents

* [Data generation and preprocesss](#data)
* [Data location and summary](#data_loc)
* [Machine learning model](#ml)
    * [FastText](#fasttext)
    * [Tencent's NeuralClassifier](#tencent)
    * [HFT-CNN](#hftcnn)
* [Evaluation](#eval)
* [Visualization](#visual)
* [Web app](#webapp)

## Data generation and preprocess <a id="sys-arc"></a>

First we generate our dataset from all granted patents up to September 2019, the total number of patent in the dataset is 4,363,544. In order to further train our model, we split our dataset into training set, validation set and testing set by the ratio of 7:2:1.

Step 2: We parse the cpc field into labels we need (section, classs, subclass, etc.), convert the text into a list of tokens, and split the data into train, valid, and test datasets. This step also removes all punctuations and convert all uppercase letters into lower case. This can be done by running the file [data_preprocess/text_preprocess.py](https://github.com/yyn19951228/Dorothy-Ymir/blob/master/data_preprocess/text_preprocess.py), for example:

```sh
$ python3 -u data_preprocess/text_preprocess.py \
/pylon5/sez3a3p/yyn1228/data/json_reparse \
/pylon5/sez3a3p/yyn1228/data/all_data
```

Step 3: We further preprocess the data into a format that can be used by the machine learning libraries. This can be done by running the file [data_preprocess/create_training_data.py](https://github.com/yyn19951228/Dorothy-Ymir/blob/master/data_preprocess/create_training_data.py). Note that the file takes 6 arguments:
* input directory
* output directory
* text field: 'title', 'abstraction', 'claims', 'brief_summary', 'description'
* level name: 'section', 'class', 'subclass', 'main_group', 'subgroup'
* whether to remove stop words: True means remove stop words
* whether to follow fasttext format: True means FastText format, False means Tecent format

For example:

```sh
$ python3 -u data_preprocess/create_training_data.py \
/pylon5/sez3a3p/yyn1228/data/all_data \
/pylon5/sez3a3p/yyn1228/data/all_summary_fasttext_group \
brief_summary main_group false true
```
## Data location and summary <a id="data_loc"></a>
Processed data after Step 1, which includes 91 files and most of which have 50,000 patents.
```sh
/pylon5/sez3a3p/yyn1228/data/json_reparse
```
Processed data after Step 2, which includes three files: train.json, valid.json, and test.json.
```sh
/pylon5/sez3a3p/yyn1228/data/all_data
```
Smaller datasets for valid and test: created by shuffling valid.json and test.json above and taking the first 60,000 records. These data have the following fields:
* all_labels: all true labels at the lowest subgroup level
* title, abstraction, claims, brief_summary, description: text split into list of tokens for various cpc text fields
```sh
/pylon5/sez3a3p/yyn1228/data/all_data_small
```
Processed data after Step 3 for brief summary and subclass level, in Tecent's format. Note that these data do not have stop words.
```sh
/pylon5/sez3a3p/yyn1228/data/all_summary_nonstop
```
Processed data after Step 3 for brief summary and all levels, in FastText's format. Note that these data include stop words.
```sh
/pylon5/sez3a3p/yyn1228/data/all_summary_fasttext_section
/pylon5/sez3a3p/yyn1228/data/all_summary_fasttext_class
/pylon5/sez3a3p/yyn1228/data/all_summary_fasttext (this is for subclass)
/pylon5/sez3a3p/yyn1228/data/all_summary_fasttext_group
/pylon5/sez3a3p/yyn1228/data/all_summary_fasttext_subgroup
```
Smaller datasets initially used for testing purposes. Note that these data were generated by legacy code and may not be easily reproduced.
```sh
/pylon5/sez3a3p/yyn1228/data/summary_only
/pylon5/sez3a3p/yyn1228/data/summary_only_fasttext
/pylon5/sez3a3p/yyn1228/data/summary_only_nonstop
```

## Machine learning model <a id="ml"></a>
This section introduces how we use various libraries to train machine learning models.
### FastText <a id="fasttext"></a>
We use [Facebook's FastText library](https://github.com/facebookresearch/fastText/tree/master/python) to train the well-known FastText model. This method first converts words into word embeddings and then average word embeddings to create the document embedding. Note that this does not consider the order of the words. To keep some information regarding the order, it includes 2-grams into the vocabulary. Because this model is relatively simple and Facebook uses a lot of tricks to speed up the training, the training can be done by using CPUs instead of GPUs in a couple of hours. To account for the hierarchical information, we borrow the idea from HFT-CNN: we first train the section level and pass the word embeddings into the next word as pretrained word embeddings.

To train FastText on PSC, first run the training job to train the data on the section level
```sh
$ sbatch model/FastText/summary_all_section/train_fasttext.job
```
Then save the word embeddings by running 
```sh
$ sbatch model/FastText/summary_all_section/bin_to_vec.job
```
And then do the same thing for the class, subclass, group, and subgroup levels. To change the hyperparameters, just edit the train.py file in the corresponding folder.
### Tencent's NeuralClassifier <a id="tencent"></a>
We use [Tencent's NeuralClassifier library](https://github.com/Tencent/NeuralNLP-NeuralClassifier) to train the classic CNN/RNN/RCNN text classification models. This model accounts for the hierarchical structure by adding a loss that is calculated based on the label tree, which forces closer leaves in the tree to have closer losses. Note that the library supports many models but we also tried the classic CNN/RNN/RCNN models. We edit some code to allow for using existing vocabulary.

A detailed README on how to train the model using NeuralClassifier is saved here: [README.md](https://github.com/yyn19951228/Dorothy-Ymir/blob/master/model/NeuralClassifier/README.md). All models are saved in the "/pylon5/sez3a3p/yyn1228/Dorothy-Ymir/model/NeuralClassifier/output/xxx/checkpoint_dir_cpc" folders on PSC.

Because there are many hyperparameters to tune, we include a summary of some of the best models we trained with the corresponding hyperparameters we used:
![tencent models](https://github.com/yyn19951228/Dorothy-Ymir/blob/master/tencent.png)


### HFT-CNN <a id="hftcnn"></a>
We also use [the HFT-CNN library](https://github.com/ShimShim46/HFT-CNN) to train another model. The idea is to train a CNN model on each level, which pass word embeddings and parameters in early layers to the CNN model on the next level. We add some code to support multi-GPU training. Follow the [instructions](https://github.com/ShimShim46/HFT-CNN) here to train the model.

## Evaluation <a id="eval"></a>

The detailed evaluation is saved in [notebooks/prob_evaluate.ipynb](https://github.com/yyn19951228/Dorothy-Ymir/blob/master/notebooks/prob_evaluate.ipynb). It also includes methods to ensemble different models. See below a summary of the model results below. The best recall at n â‰ˆ 5 is 91.5%.
![evaluation](https://github.com/yyn19951228/Dorothy-Ymir/blob/master/eval.png)

## Visualization <a id="visual"></a>



## Web app <a id="webapp"></a>
Inorder to obtain an intuitive feeling of the result, we built an web app that could predict the corresponding CPC code given by any text in real time, and generate an tree plot. 

The backend for our Web app is Django, the frontend is built by React and the project is deployed on the AWS. The user could easily type in any text the describe one tech utility, and the predicted cpc codes will be render in the form of tree in secondes. 

For the backend, we load the model and make the prediction in [`models.py `](https://github.com/yyn19951228/Dorothy-Ymir/blob/visualization/visualization/visualization-demo/backend/demo/models.py) at the very first time of the prediction, so for the following predictions, we can get rid of loading the giant model file too much times. 
But the prediction also need to be structed and parsed into the format that could be used for frontend, and this part of work is done by [`treebuilders.py`](https://github.com/yyn19951228/Dorothy-Ymir/blob/visualization/visualization/visualization-demo/backend/demo/treebuilder.py) and [`views.py`](https://github.com/yyn19951228/Dorothy-Ymir/blob/visualization/visualization/visualization-demo/backend/demo/views.py).
Also, in the backend, we need to rank the predcitions according to their confidence scores made by our model for the frontend render work, so the total data we return back to front is :
```
res = {'tree': tree, 'ordered_labels': ordered_labels}
```
where `tree` is the parsed predictions in tree structure format, and `ordered_labels` is the prediction labels ranked by their confidence scores.

In the front end, we use these two data to render a tree chart, and we also provide an adjust bar that could change the tree leafnode numbers for analyse.


The implementation is well documented, so it is easy for further integration.  